package com.ms.banking.pbg_analytics_spark.nii.process

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.Row
import org.apache.spark.sql.SQLContext
import org.slf4j.Logger
import org.slf4j.LoggerFactory
import com.ms.banking.pbg_analytics_spark.ccar_mortgage.job.constants.CCARConstants
import com.ms.banking.pbg_analytics_spark.ccar_mortgage.job.utility.Utils
import com.ms.banking.pbg_analytics_spark.nii.model.MovAvgArrayList
import com.ms.banking.pbg_analytics_spark.nii.model.NIIConfig
import com.ms.banking.pbg_analytics_spark.nii.model.NIIRegression
import org.apache.spark.SparkContext
import org.apache.spark.sql.types.DoubleType
import org.apache.spark.sql.types.StructField

/**
 * class to generate input data for nii regression process
 *
 * @author venreddy
 */

class NIIRegressionInputRetriever extends Serializable {
  val logger: Logger = LoggerFactory.getLogger(getClass.getName)
  val utils = Utils.getccarUtils()
  val prop = utils.getProperties

  /**
   * @param niiConfig
   * @return
   */
  def apply(niiConfig: NIIConfig): (DataFrame, SparkContext) = {
    val sc = utils.getSparkContext()
    val sqlContext = new SQLContext(sc)
    val map: Map[String, List[NIIRegression]] = null
    val movingAvg5 = new Array[MovAvgArrayList](6)
    val movingAvg90 = new Array[MovAvgArrayList](6)
    for (i <- 0 to movingAvg5.length - 1) {
      movingAvg5(i) = new MovAvgArrayList(5)
      movingAvg90(i) = new MovAvgArrayList(90)
    }
    val movingpAvg5 = new Array[MovAvgArrayList](7)
    for (i <- 0 to movingpAvg5.length - 1) {
      movingpAvg5(i) = new MovAvgArrayList(5)
    }
    logger.info("Creating objects for Retriving Rates")
    val niiStratsModelRatesRetriever = new NIIStratsModelRatesRetriever
    val niiStratsProjRatesRetriever = new NIIStratsProjRatesRetriever
    val niiStratsHistRatesRetriever = new NIIStratsHistRatesRetriever
    val niiLoanDWRetriever = new NIILoanDWRetriever
    val niiPricerRateRetriever = new NIIPricerRateRetriever

    val niiModelList = niiStratsModelRatesRetriever.apply(sc, sqlContext, prop, niiConfig).cache()
    val niiProjList = niiStratsProjRatesRetriever.apply(sc, sqlContext, prop, niiConfig).cache()
    val niiHIstList = niiStratsHistRatesRetriever.apply(sc, sqlContext, prop, niiConfig).cache()
    val niiLoanList = niiLoanDWRetriever.apply(sc, sqlContext, prop, niiConfig).cache()
    val niiPricerList = niiPricerRateRetriever.apply(sc, sqlContext, prop, niiConfig).cache

    /*    niiModelList.rdd.repartition(1).map { x => x.mkString(",") }.saveAsTextFile("niiOutput/modelout2")
    niiProjList.rdd.repartition(1).map { x => x.mkString(",") }.saveAsTextFile("niiOutput/projout2")
    niiHIstList.rdd.repartition(1).map { x => x.mkString(",") }.saveAsTextFile("niiOutput/histout2")*/

    niiModelList.registerTempTable("niiModelListTable")
    niiProjList.registerTempTable("niiProjListTable")
    niiHIstList.registerTempTable("niiHIstListTable")

    var combinedRates = sqlContext.sql(CCARConstants.UNION_STRATE_RATES)
    /*  var combinedRates = niiModelList.unionAll(niiHIstList).unionAll(niiProjList)*/
    //  combinedRates.rdd.repartition(1).map { x => x.mkString(",") }.saveAsTextFile("niiOutput/union")

    combinedRates = combinedRates.repartition(3, combinedRates.col("scenario"))
    combinedRates.registerTempTable("combinedRates")

    combinedRates.show(8)
    println("union all")
    combinedRates = sqlContext.sql(CCARConstants.COMBINED_RATES)
    //combinedRates = combinedRates.repartition(16)

    // println("Before moving average partitions : " + combinedRates.rdd.partitions.length)
    combinedRates.show(24)
    logger.info("Calculating Moving Average 5 and 90")
    var combinedRatesAvgRDD = combinedRates.map { r =>
      {
        //println("Elements size :" + movingAvg90(0).elements.size())
        for (i <- 0 to movingAvg5.length - 1) {
          movingAvg90(i).add(r.getDouble(i + 3))
          movingAvg5(i).add(r.getDouble(i + 3))
          if (i == 0) {
            for (i <- 0 to movingAvg90(0).elements.size() - 1) {
              //   println(r.getString(0) + ":" + r.getString(1) + ":" + r.getString(2) + ":" + movingAvg90(0).elements.get(i) + ":" + movingAvg90(0).getMean())
            }
          }
        }

        Row(r.getString(0), r.getString(1), r.getString(2),
          r.getDouble(8),
          movingAvg90(0).getMean(),
          movingAvg90(1).getMean(),
          movingAvg90(2).getMean(),
          movingAvg90(3).getMean(),
          movingAvg90(4).getMean(),
          movingAvg90(5).getMean(),
          movingAvg5(0).getMean(),
          movingAvg5(1).getMean(),
          movingAvg5(2).getMean(),
          movingAvg5(3).getMean(),
          movingAvg5(4).getMean(),
          movingAvg5(5).getMean())
      }
    }

    combinedRates = sqlContext.createDataFrame(combinedRatesAvgRDD, utils.createStructSchema(CCARConstants.NII_MOVAVG_SCHEMA))
    combinedRates.show(25)
    println("After moving average")

    logger.info("1.4: Merge strats rates, product shares and pricer rates together")
    combinedRates.registerTempTable("stratesrates")
    niiLoanList.registerTempTable("loantab")
    niiPricerList.registerTempTable("pricertab")

    var merge_df = sqlContext.sql(CCARConstants.MERGE_RATES)
    
    logger.info("1.5: Split all inputs into historical ")
    val split_hist = merge_df.filter(merge_df.col("scenarioPeriod").equalTo("hist"))
    val split_proj = merge_df.filter(merge_df.col("scenarioPeriod").equalTo("proj"))

    split_hist.show(10)
    println("split_hist")
    split_proj.show(18)
    println("split_proj")
    val split_hist_rdd = split_hist.map { r =>
      {
        //println("Elements size :" + movingAvg90(0).elements.size())
        for (i <- 0 to movingpAvg5.length - 1) {
          movingpAvg5(i).add(r.getDouble(i + 15))
          if (i == 0) {
            for (i <- 0 to movingpAvg5(0).elements.size() - 1) {
              //   println(r.getString(0) + ":" + r.getString(1) + ":" + r.getString(2) + ":" + movingAvg90(0).elements.get(i) + ":" + movingAvg90(0).getMean())
            }
          }
        }

        Row(r.getString(0), r.getString(1), r.getString(2),r.getDouble(3),r.getDouble(4),r.getDouble(5),r.getDouble(6),r.getDouble(7),
            r.getDouble(8),r.getDouble(9),r.getDouble(10),r.getDouble(11),r.getDouble(12),r.getDouble(13),r.getDouble(14),r.getDouble(15),
          r.getDouble(16),r.getDouble(17),r.getDouble(18),r.getDouble(19),r.getDouble(20),r.getDouble(21),
          
          movingpAvg5(0).getMean(),
          movingpAvg5(1).getMean(),
          movingpAvg5(2).getMean(),
          movingpAvg5(3).getMean(),
          movingpAvg5(4).getMean(),
          movingpAvg5(5).getMean(),
          movingpAvg5(6).getMean())
          
      }
    }
    var hist_schema = split_hist.schema.add(StructField("Ft_01m_spr_mavg5", DoubleType, true) )
    .add(StructField("Ft_03y_spr_avg5", DoubleType, true) )
    .add(StructField("Ft_05y_spr_mavg5", DoubleType, true) )
    .add(StructField("Ft_07y_spr_mavg5", DoubleType, true) )
    .add(StructField("Ft_10y_spr_mavg5", DoubleType, true) )
    .add(StructField("Fx_15y_spr_mavg5", DoubleType, true) )
    .add(StructField("Fx_30y_spr_mavg5", DoubleType, true) )
val split_hist_df=sqlContext.createDataFrame(split_hist_rdd, hist_schema)
    /*combinedRates.rdd.repartition(1).map { x => x.mkString(",") }.saveAsTextFile("niiOutput/combi2")
    merge_df.rdd.repartition(1).map { x => x.mkString(",") }.saveAsTextFile("niiOutput/merge4")*/

    split_hist_df.show(18)
    split_hist_df.printSchema()
    println("split_hist_df:")

    return (niiModelList, sc)
  }

}
--------------
