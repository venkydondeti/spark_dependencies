package com.ms.banking.pbg_analytics_spark.nii.process

import com.ms.banking.pbg_analytics_spark.nii.model.NIIConfig
import com.ms.banking.pbg_analytics_spark.nii.model.NIIRegression
import java.util.Properties
import org.slf4j.Logger
import org.apache.spark.sql.DataFrame
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import com.ms.banking.pbg_analytics_spark.ccar_mortgage.job.utility.Utils
import org.slf4j.LoggerFactory
import com.ms.banking.pbg_analytics_spark.nii.model.MovAvgArrayList
import org.apache.commons.math3.stat.regression.SimpleRegression
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.DoubleType
import org.apache.spark.sql.types.StructField

/**
 * @author venreddy
 */
class NIIRegressionTransformer {

  val logger: Logger = LoggerFactory.getLogger(getClass.getName)
  val utils = Utils.getccarUtils()
  val prop = utils.getProperties

  /**
   * @param sc
   * @param sqlContext
   * @param prop
   * @param niiConfig
   * @return
   */
  def apply(sc: SparkContext, sqlContext: SQLContext, prop: Properties, niiConfig: NIIConfig, split_hist_df: DataFrame): DataFrame = {
    split_hist_df.printSchema()
    val simpleRegressionArr = new Array[SimpleRegression](7)
    for (i <- 0 to simpleRegressionArr.length - 1) {
      simpleRegressionArr(i) = new SimpleRegression()
      simpleRegressionArr(i).addData(1.0, 1.0)
    }
    val hist_reg_rdd = split_hist_df.map { r =>
      {
        for (i <- 0 to simpleRegressionArr.length - 1) {
          simpleRegressionArr(i).addData(r.getDouble(i + 16), r.getDouble(3))
        }

        Row(r.getString(0), r.getString(1), r.getString(2), r.getDouble(3), r.getDouble(4), r.getDouble(5), r.getDouble(6), r.getDouble(7),
          r.getDouble(8), r.getDouble(9), r.getDouble(10), r.getDouble(11), r.getDouble(12), r.getDouble(13), r.getDouble(14), r.getDouble(15),
          r.getDouble(16), r.getDouble(17), r.getDouble(18), r.getDouble(19), r.getDouble(20), r.getDouble(21), r.getDouble(22),
          r.getDouble(23), r.getDouble(24), r.getDouble(25), r.getDouble(26), r.getDouble(27), r.getDouble(28), r.getDouble(29),
          simpleRegressionArr(0).getSlope,
          simpleRegressionArr(1).getSlope,
          simpleRegressionArr(2).getSlope,
          simpleRegressionArr(3).getSlope,
          simpleRegressionArr(4).getSlope,
          simpleRegressionArr(5).getSlope,
          simpleRegressionArr(6).getSlope)

      }
    }
    val reg_schema = split_hist_df.schema.add(StructField("R1_Ft_01m", DoubleType, true))
      .add(StructField("R1_Ft_03y", DoubleType, true))
      .add(StructField("R1_Ft_05y", DoubleType, true))
      .add(StructField("R1_Ft_07y", DoubleType, true))
      .add(StructField("R1_Ft_10y", DoubleType, true))
      .add(StructField("R1_Fx_15y", DoubleType, true))
      .add(StructField("R1_Fx_30y", DoubleType, true))
    val hist_reg_df = sqlContext.createDataFrame(hist_reg_rdd, reg_schema)
    hist_reg_df.show(8)
    println("hist_reg_df showing.......")
    return hist_reg_df
  }
}
======
#!/ms/dist/fsf/PROJ/ksh/93u/bin/ksh -p

export RUNSCRIPT_NAME=$0
##############################
#Print help Function
##############################
function printHelp
{
  print "usage:\n" >&2
  print "./${RUNSCRIPT_NAME} " >&2
  print "\t-r\t-- Remote debug enabled on port @debug.remoteport@" &>2
  print "\t-p\t-- JProfiler extensions enabled" &>2
  print "\t-e\t-- Echo Command only.  Do not run!" &>2
  print "\t-h\t-- Prints this help message" &>2
}
if [ "$#" -ne 1 ]; then
  printHelp 
  exit 21
fi
print "Loading hadoop module"
#module load @hadoop/dzhadoop19

print "Creating jar paths"
jars=`echo /ms/dist/ossjava/PROJ/spring/4.2.6/common/lib/*.jar | tr ' ' ','`


#spark-submit --master yarn-client --num-executors 12 --driver-memory 8G --executor-memory 8G --class com.ms.banking.pbg_analytics_spark.ccar_mortgage.job.aggregator.AggregateMTG --jars //ms/dist/banking/PROJ/pricer_common/dev-2016.06.20-1/common/lib/pricer_common.jar,${jars} /v/region/na/appl/banking/pbg_risk_analytics/data/dev/unittest/ccar_spark/FwdRisk.jar $1

spark-submit --master yarn-client --num-executors 12 --driver-memory 8G --executor-memory 8G --class com.ms.banking.pbg_analytics_spark.nii.process.CCARNIIProcess --jars //ms/dist/banking/PROJ/pricer_common/dev-2016.06.20-1/common/lib/pricer_common.jar,${jars} /v/region/na/appl/banking/pbg_risk_analytics/data/dev/unittest/ccar_spark/FwdRisk.jar $1

rc=$? 
if [[ $rc != 0 ]];  then
  exit 1
fi
exit 0

=========
